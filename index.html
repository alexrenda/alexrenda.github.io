<html>
  <head>
    <title>Alex&nbsp;Renda</title>
    <link rel="stylesheet" type="text/css" href="css/style.css" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body style="margin:1em">
    <div class="frontmatter">
      <h1 id="name">Alex&nbsp;Renda</h1>
      <img src="img/headshot.jpg" id="headshot"/>
      <div>
        <div id="shortbio">
          Grad Student @ MIT CSAIL.
          <br/>
          Programming Systems & Machine Learning.
        </div>

        <br/>
        <table>
          <tr>
            <td>Email:</td> <td><a href="mailto:renda@csail.mit.edu">renda@csail.mit.edu</a></td>
          </tr>
          <tr>
            <td>Twitter:</td> <td><a href="https://twitter.com/alex_renda_">@alex_renda_</a></td>
          </tr>
          <tr>
            <td>GitHub:</td> <td><a href="https://github.com/alexrenda">@alexrenda</a></td>
          </tr>
          <tr>
            <td>LinkedIn:</td> <td><a href="https://linkedin.com/in/alexrenda">@alexrenda</a></td>
          </tr>
          <tr>
            <td>Google Scholar:</td> <td><a href="https://scholar.google.com/citations?hl=en&user=4BCuJ2AAAAAJ">Alex&nbsp;Renda</a></td>
          </tr>
          <tr>
            <td>Office:</td> <td>MIT 32G-738, Cambridge MA 02139</td>
          </tr>
        </table>
      </div>
    </div>
    <br/>

    <div style="clear:right">
      <h2>About</h2>
      I'm interested in using machine learning as an abstraction to help write complex programs.
      I want to make it easier to write programs that are hard or even impossible to write by hand.
      I also want to make machine learning more efficient, to be able to incorporate learning into more programs.

      <br/><br/>

      <a href="pdf/cv.pdf">Here is a full CV</a>.
    </div>

    <h2>News</h2>
    <table id="news">
      <tr><td>July, 2021</td> <td><i>Programming with Surrogates of Programs</i> accepted at <a href="https://2021.splashcon.org/track/splash-2021-Onward-papers">Onward!&nbsp;2021</a>.</tr>
      <tr><td>July, 2020</td> <td><i>DiffTune: Optimizing CPU Simulator Parameters with Learned Differentiable Surrogates</i> accepted at <a href="https://www.microarch.org/micro53/">MICRO&nbsp;2020</a>.</tr>
      <tr><td>June, 2020</td> <td>I'm spending the summer at <a href="https://www.octoml.ai">OctoML</a>, working on reducing training time for neural network approximations.</tr>
      <tr><td>April, 2020</td> <td><i>Comparing Rewinding and Fine-tuning in Neural Network Pruning</i> published at  <a href="https://www.iclr.cc/Conferences/2020">ICLR&nbsp;2020</a>. <a href="http://news.mit.edu/2020/foolproof-way-shrink-deep-learning-models-0430">MIT&nbsp;News</a>. <a href="https://mitibmwatsonailab.mit.edu/research/blog/comparing-rewinding-and-fine-tuning-in-neural-network-pruning/">IBM Research blog</a>.</tr>
    </table>


    <h2>Publications</h2>

    <ul>
      <li>
        <i>DiffTune: Optimizing CPU Simulator Parameters with Learned Differentiable Surrogates.</i>
        <br/>
        <b>Alex&nbsp;Renda</b>, Yishen Chen, Charith Mendis, Michael Carbin.
        <br/>
        <a class="conferencelink" href="https://www.microarch.org/micro53/">MICRO, 2020</a>.
        <a href="https://arxiv.org/abs/2010.04017">Paper</a>.
        <a href="https://github.com/ithemal/DiffTune">Code</a>.
        <a href="https://youtu.be/7sN2YsqgPLY">Presentation</a>.
        <a href="bibtex/difftune-renda-2020.txt">Bibtex</a>.
      </li>
      <li>
        <i>TIRAMISU: A Polyhedral Compiler for Dense and Sparse Deep Learning.</i>
        <br/>
        Riyadh Baghdadi, Abdelkader Nadir Debbagh, Kamel Abdous, Fatima Zohra Benhamida, <b>Alex&nbsp;Renda</b>, Jonathan Elliott Frankle, Michael Carbin, Saman Amarasinghe.
        <br/>
        <a class="conferencelink" href="http://learningsys.org/neurips19/">Workshop on Systems for ML, NeurIPS 2019</a>.
        <br/>
        <a href="https://arxiv.org/abs/2005.04091">Extended Draft</a>.
        <a href="bibtex/baghdadi-tiramisu-2019.txt">Bibtex</a>.
      </li>
      <li>
        <i>Comparing Rewinding and Fine-tuning in Neural Network Pruning.</i>
        <br/>
        <b>Alex&nbsp;Renda</b>, Jonathan Frankle, Michael Carbin.
        <br/>
        <a class="conferencelink" href="https://iclr.cc/Conferences/2020">ICLR, 2020</a>.
        <a href="https://arxiv.org/abs/2003.02389">Paper</a>.
        <a href="https://github.com/lottery-ticket/rewinding-iclr20-public">Code</a>.
        <a href="http://youtube.com/watch?v=GZYihBIwebE">Presentation</a>.
        <a href="bibtex/comparing-renda-2020.txt">Bibtex</a>.
        <br/>
        <span class="award">Oral presentation (&lt;2% of submitted papers).</span>

      </li>

      <li>
        <i>BHive: A Benchmark Suite and Measurement Framework for Validating x86-64 Basic Block Performance Models</i>.
        <br/>
        Yishen Chen, Ajay Brahmakshatriya, Charith Mendis, <b>Alex&nbsp;Renda</b>, Eric Atkinson, Ond&#x0159;ej S&#x00FD;kora, Saman Amarasinghe, Michael Carbin.
        <br/>
        <a class="conferencelink" href="http://www.iiswc.org/iiswc2019/index.html">IISWC, 2019</a>.
        <a href="https://groups.csail.mit.edu/commit/papers/19/ithemal-measurement.pdf">Paper</a>.
        <a href="https://github.com/ithemal/bhive">Code</a>.
        <a href="bibtex/chen-bhive-2019.txt">Bibtex</a>.
      </li>

      <li>
        <i>Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural&nbsp;Networks</i>.
        <br/>
        Charith Mendis, <b>Alex&nbsp;Renda</b>, Saman Amarasinghe, Michael Carbin.
        <br/>
        <a class="conferencelink" href="http://www.icml.cc/Conferences/2019">ICML, 2019</a>.
        <a href="https://arxiv.org/abs/1808.07412">Paper</a>.
        <a href="https://github.com/ithemal/ithemal">Code</a>.
        <a href="bibtex/mendis-ithemal-2019.txt">Bibtex</a>.
        <br/>
        <span class="award">Best Paper award at the <a class="conferencelink" href="http://mlforsystems.org/isca2019/">ML for Systems workshop</a> at ISCA 2019.
      </li>

      <li>
        <i>Programming Language Support for Natural Language Interaction</i>
        <br/>
        <b>Alex&nbsp;Renda</b>, Harrison Goldstein, Sarah Bird, Chris Quirk, Adrian Sampson.
        <br/>
        <a class="conferencelink" href="http://mlsys.org/Conferences/2018/">SysML, 2018</a>.
        <a href="http://mlsys.org/Conferences/doc/2018/56.pdf">Abstract</a>.
        <a href="https://arxiv.org/abs/1709.04991">Extended draft</a>.
        <a href="https://github.com/cucapra/opal">Code</a>.
        <a href="bibtex/renda-opal-2018.txt">Bibtex</a>.
      </li>
    </ul>

    <h2>Honors</h2>
    <ul>
      <li>
        NSF GRFP Honorable Mention, 2020
      </li>
      <li>
        Best Paper award for Ithemal at the ML for Systems workshop at ISCA 2019
      </li>
      <li>
        MIT Great Educators Fellowship, 2018-2019
      </li>
      <li>
        Cornell University: Summa Cum Laude with Honors, 2018
      </li>
    </ul>

    <h2>Academic Service</h2>
    <ul>
      <li>OOPSLA 2021 &mdash; Artifact Evaluator </li>
      <li>NeurIPS 2021 &mdash; Reviewer</li>
      <li>ICML 2021 &mdash; Reviewer</li>
      <li>ASPLOS 2021 &mdash; Artifact Evaluator</li>
      <li>ICLR 2021 &mdash; Reviewer (Outstanding Reviewer)</li>
      <li>AAAI 2021 &mdash; Emergency Reviewer</li>
      <li>NeurIPS 2020 &mdash; Reviewer</li>
      <li>ICML 2020 &mdash; Reviewer (Top 33% Reviewer)</li>
    </ul>

    <h2>Institutional Service</h2>
    <ul>
      <li>PLSE Seminar Co-Coordinator &mdash; Spring 2021&ndash;present </li>
      <li>PLSE Coffee Chat Co-Coordinator &mdash; Fall 2020&ndash;present </li>
      <li>EECS GAAP Mentor &mdash; Fall 2020, Fall 2021</li>
      <li>PLSE Lunch Co-Coordinator &mdash; Fall 2019&ndash;Spring 2020 </li>
      <li>Fast ML Reading Group Coordinator &mdash; Fall 2019&ndash;Spring 2020</li>
    </ul>

    <!-- <h2>Invited Talks</h2> -->
    <!-- <ul> -->
    <!--   <li> -->
    <!--     Fall 2020 &mdash; Facebook &mdash; <i>DiffTune: Optimizing CPU Simulator Parameters with Learned Differentiable Surrogates</i> -->
    <!--   </li> -->
    <!--   <li> -->
    <!--     Summer 2020 &mdash; OctoML &mdash; <i>DiffTune: Optimizing CPU Simulator Parameters with Learned Differentiable Surrogates</i> -->
    <!--   </li> -->
    <!-- </ul> -->


    <h2>Teaching</h2>
    <ul>
      <li>
        CS 4120 &mdash; Introduction to Compilers.
        Teaching Assistant. Cornell University, Spring 2018.
      </li>
      <li>
        CS 2112 &mdash; Object Oriented Programming and Data Structures - Honors.
        Consultant. Cornell University, Fall 2016, Fall 2015.
      </li>
    </ul>

    <h2>Education</h2>
    <ul>
      <li>
        Ph.D. student in EECS at MIT. 2018-present.
        <br/>
        Advised by <a href="https://people.csail.mit.edu/mcarbin/">Michael Carbin</a>.
      </li>

      <li>
        Master of Science in Electrical Engineering and Computer Science, MIT, May 2020.
        <br/>
        <a href="pdf/sm-thesis.pdf"><i>Comparing Rewinding and Fine-tuning in Neural Network Pruning</i></a>.
        <br/>
        Advised by <a href="https://people.csail.mit.edu/mcarbin/">Michael Carbin</a>.
      </li>
      <li>
        Bachelor of Science (Summa Cum Laude, with Honors) in Computer Science with a minor in Linguistics, Cornell&nbsp;University, May 2018.
        <br/>
        Worked with <a href="http://adriansampson.net">Adrian Sampson</a>.
      </li>
    </ul>

    <h2>Industry Experience</h2>
    <ul>
      <li>
        Summer 2020: MLSys Intern at OctoML
      </li><li>
        Summer 2018: Software Engineering Intern at Two Sigma
      </li><li>
        Summer 2017: Software Engineering Intern at Two Sigma
      </li><li>
        Summer 2016: Software Engineering Intern at Facebook
      </li><li>
        Summer 2014: System Validation Intern at Tesla
      </li>
    </ul>

  </body>
</html>
